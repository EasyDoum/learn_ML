# 机器学习中的“学习”是什么
***学习***一词在机器学习的文献中使用频率很高；毕竟，它甚至是“机器学习”这个词的一部分。Dual等人写道：
> 从广义上讲，任何在分类器射击中结合训练样本信息的方法都采用了学习。

Bishop给出了（图像）分类背景下学习的定义：
> 运行机器学习算法的结果可以表示为一个函数**y(x)**，该函数讲新的数字图像**x**作为输入，并生成一个输出向量**y**，其编码方式与目标向量相同。

Hastie等人是这样说的：
> 许多领域都在产生大量的数据，统计学家的工作就是让这些数据变得有意义：提取出重要的模式和趋势，并理解“数据说明了什么”。我们称之为从数据中学习。

在第1节介绍的术语的通用性为机器学习中的学习提供了一种简单而统一的定义方法：
> 学习是指*利用数据估计从输入空间到输出空间的映射或映射的各个参数。*

&emsp;&emsp;
该定义既不过于具体到某些特定的任务，也不会过于宽泛以免产生歧义。我们所说的“数据”是指通过测量、观察、查询或先验知识获得的任何信息的集合。这样，我们就可以把用于分类、回归、降维、聚类、强化学习等的任何映射的估算过程称为学习。因此，学习到的映射只是一个***估计器***。此外，算法的效用也隐含在这一定义中；毕竟，数据应该用在算法中来估计映射。

&emsp;&emsp;
有人可能会说，我们可以将学习定义为机器学习中可能出现的任何估计。虽然这样做并不存在认知上的困难，但这种笼统的定义会使其变得过于宽泛，无法用于指代机器学习中的有用任务。为了正确的看待这个问题，假设我们要开发一种分类器，能将给定的图像分类为猫或狗。如果使用如此宽泛的学习定义，那么即使是通过抛硬币的方法来给图像分配标签，也算是发生了学习，因为我们是在估计给定数据（图像）的标签；但问题是，这种“学习”对于重新理解输入和输出空间之间的关系是否有价值。换句话说，虽然我们的目标是估计标签，但学习本身并不是目标，而是实现目标的过程。

&emsp;&emsp;
在现阶段，似乎需要简单讨论一下先验知识在学习过程中的影响和作用。***先验知识***指的是在学习过程中可以使用的在观察用于估计从输入到输出空间的映射的随机变量的实现之前就可以获得的任何信息。假设在上面的图像分类示例中，我们被告知将收集图像的群体（输入空间）包含90%的狗和仅10%的猫。假设现在收集了数据，但给定训练数据中的类别（猫和狗）比例相同。如果分别对不同类别进行采样，那么总体和给定样本之间的类别比例差异就很常见。我们从给定的训练数据中训练出了4个分类器，然后根据这4个分类器的多数票来决定是否给给定图像分配标签，这就是所谓的集成学习（第8章将讨论），我们将其他几个模型（即基础模型）的结果结合起来。不过，在这里，在偶数个分类器之间进行多数票表决意味着个别分类器分配的猫和狗之间会出现平局（例如，2个分类器分配猫，2个分类器分配狗）。打破这种平局的一种方法是随机分配一个标签。然而，根据给定的先验知识，这里更好的策略是将任何出现平局的图像分配给狗，因为它发生的概率是猫的9倍。在学习集成分类器的过程中，我们使用联合输入输出空间的实例以及先验知识。